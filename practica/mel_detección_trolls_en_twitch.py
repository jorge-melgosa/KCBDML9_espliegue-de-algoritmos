# -*- coding: utf-8 -*-
"""MEL - Detección Trolls en Twitch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17x7eRFHuz3ETJs-zvQPdXOfZjw5UWqBI

# Práctica Final: detección de mensajes troll en chat de Twitch en tiempo real

Durante este último año la plataforma de vídeo en streaming Twitch ha cogido mucha popularidad debido a la situación que hemos vivido debido al COVID-19. Por esto, mucha gente de todas las edades ha empezado a consumir esta plataforma de manera diaria.

Como consecuencia, no sólo han aumentado las personas que ven contenido en Twitch, sino también el número de los denominados *trolls*, gente que pone comentarios ofensivos en los chat de los streamers.

En esta práctica se desarrollará un sistema autónomo basado en IA y desplegado en GCP que detectará en tiempo real si los mensajes que se envían a un canal de Twitch son de un *troll* o no. La práctica constará de tres partes principales que serán evaluadas en la corrección:
1. Entrenamiento e inferencia en Batch de un modelo usando Dataflow y AI Platform. **(3.5 puntos)**.
2. Despliegue e inferencia online en microservicio con el modelo. **(3.5 puntos)**.
3. Inferencia en streaming de un canal de Twitch con el microservicio anterior. **(3 puntos)**.

# Configuración de nuestro proyecto en GCP
"""

PROJECT_ID = "kc-mel-practica-da" #@param {type:"string"}
! gcloud config set project $PROJECT_ID

# Commented out IPython magic to ensure Python compatibility.
import sys

# If you are running this notebook in Colab, run this cell and follow the
# instructions to authenticate your GCP account. This provides access to your
# Cloud Storage bucket and lets you submit training jobs and prediction
# requests.

if 'google.colab' in sys.modules:
  from google.colab import auth as google_auth
  google_auth.authenticate_user()

# If you are running this notebook locally, replace the string below with the
# path to your service account key and run this cell to authenticate your GCP
# account.
else:
#   %env GOOGLE_APPLICATION_CREDENTIALS ''

BUCKET_NAME = "kc-mel-practica-da" #@param {type:"string"}
REGION = "europe-west1" #@param {type:"string"}

! gsutil ls -al gs://$BUCKET_NAME

"""# Entrenamiento e inferencia en Batch

Para esta primera parte se va a utilizar [Tweets Dataset for Detection of Cyber-Trolls](https://www.kaggle.com/dataturks/dataset-for-detection-of-cybertrolls). El objetivo es desarrollar un clasificador binario para detectar si el mensaje recibido es troll (1) o no (0). **Las métricas obtenidas del entrenamiento y la inferencia no se tendrán en cuenta para la evaluación de la práctica, la importancia está en la arquitectura de la solución**.

A continuación os dejo un diagrama con la arquitectura que se va a desarrollar:

![batch_diagram](https://drive.google.com/uc?export=view&id=1h1BkIunyKSkJYFRbXKNWpHOZ_rDUyGAT)

A continuación, se van a subir los datos de entrenamiento al bucket del proyecto que se haya creado. **Importante:** crea el bucket en una única región para evitar problemas más adelante.
"""

# Upload data to your bucket
! wget https://storage.googleapis.com/twitch-practice-keepcoding/data.json -O - | gsutil cp - gs://$BUCKET_NAME/data.json

"""Ahora se crea el directorio dónde vas a desarrollar esta primera parte de la práctica."""

# Commented out IPython magic to ensure Python compatibility.
# %mkdir /content/batch

"""Se establece el directorio de trabajo que hemos creado."""

# Commented out IPython magic to ensure Python compatibility.
import os

# Set the working directory to the sample code directory
# %cd /content/batch

WORK_DIR = os.getcwd()

"""Ahora se descargarán los datos en el workspace de Colab para trabajar en local."""

! wget https://storage.googleapis.com/twitch-practice-keepcoding/data.json

"""Se establecen las dependencias que se usarán en la práctica. Se pueden añadir y quitar las dependencias que no se usen o viceversa."""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile requirements.txt
# 
# apache-beam[gcp]==2.24.0
# tensorflow
# gensim==3.6.0
# fsspec==0.8.4
# gcsfs==0.7.1
# numpy==1.20.0

"""Instalamos las dependencias. **No olvidarse de reiniciar el entorno al instalar y establecer las variables y credenciales de GCP al arrancar.**"""

! pip install -r requirements.txt

"""##**Entreglable (0.5 puntos)**

Desarrollar un pipeline de preprocesamiento utilizando Apache Beam para generar datos de train, eval y test para los datos proporcionados anteriormente. Requisitos:

- Proporcionar dos modos de ejecución: `train` y `test`
- Soportar ejecuciones en local con `DirectRunner` y ejecuciones en Dataflow usando `DataFlowRunner`.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile preprocess.py
# 
# #
# # Licensed to the Apache Software Foundation (ASF) under one or more
# # contributor license agreements.  See the NOTICE file distributed with
# # this work for additional information regarding copyright ownership.
# # The ASF licenses this file to You under the Apache License, Version 2.0
# # (the "License"); you may not use this file except in compliance with
# # the License.  You may obtain a copy of the License at
# #
# #    http://www.apache.org/licenses/LICENSE-2.0
# #
# # Unless required by applicable law or agreed to in writing, software
# # distributed under the License is distributed on an "AS IS" BASIS,
# # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# # See the License for the specific language governing permissions and
# # limitations under the License.
# #
# 
# """A word-counting workflow."""
# 
# # pytype: skip-file
# 
# from __future__ import absolute_import
# 
# import argparse
# import logging
# import re
# import os
# import json
# import random
# 
# from past.builtins import unicode
# 
# import apache_beam as beam
# from apache_beam.io import ReadFromText
# from apache_beam.io import WriteToText
# from apache_beam.coders.coders import Coder
# from apache_beam.options.pipeline_options import PipelineOptions
# from apache_beam.options.pipeline_options import SetupOptions, DirectOptions
# 
# import nltk
# from nltk.corpus import stopwords
# from nltk.stem import SnowballStemmer
# 
# nltk.download("stopwords")
# 
# # CLEANING
# STOP_WORDS = stopwords.words("english")
# STEMMER = SnowballStemmer("english")
# TEXT_CLEANING_RE = "@\S+|https?:\S+|http?:\S|[^A-Za-z0-9]+"
# 
# 
# class ExtractColumnsDoFn(beam.DoFn):
#     def process(self, element):
#         # space removal
#         element_split = json.loads(element)
#         # text, troll/person
#         yield element_split['content'], element_split['annotation']['label'][0]
# 
# 
# class PreprocessColumnsTrainFn(beam.DoFn):
#     def process_troll(self, troll):
#       troll_ = int(troll)
#       if troll_ == 0:
#           return "PERSON"
#       else:
#           return "TROLL"
# 
#     def process_text(self, text):
#         # Remove link,user and special characters
#         stem = False
#         text = re.sub(TEXT_CLEANING_RE, " ", str(text).lower()).strip()
#         tokens = []
#         for token in text.split():
#             if token not in STOP_WORDS:
#                 if stem:
#                     tokens.append(STEMMER.stem(token))
#                 else:
#                     tokens.append(token)
#         return " ".join(tokens)
# 
#     def process(self, element):
#         processed_text = self.process_text(element[0])
#         processed_troll = self.process_troll(element[1])
#         yield f"{processed_text}, {processed_troll}"
# 
# 
# class CustomCoder(Coder):
#     """A custom coder used for reading and writing strings"""
# 
#     def __init__(self, encoding: str):
#         # latin-1
#         # iso-8859-1
#         self.enconding = encoding
# 
#     def encode(self, value):
#         return value.encode(self.enconding)
# 
#     def decode(self, value):
#         return value.decode(self.enconding)
# 
#     def is_deterministic(self):
#         return True
# 
# 
# def run(argv=None, save_main_session=True):
# 
#     """Main entry point; defines and runs the wordcount pipeline."""
# 
#     parser = argparse.ArgumentParser()
# 
#     parser.add_argument(
#         "--work-dir", dest="work_dir", required=True, help="Working directory",
#     )
# 
#     parser.add_argument(
#         "--input", dest="input", required=True, help="Input dataset in work dir",
#     )
#     parser.add_argument(
#         "--output",
#         dest="output",
#         required=True,
#         help="Output path to store transformed data in work dir",
#     )
#     parser.add_argument(
#         "--mode",
#         dest="mode",
#         required=True,
#         choices=["train", "test"],
#         help="Type of output to store transformed data",
#     )
# 
#     known_args, pipeline_args = parser.parse_known_args(argv)
# 
#     # We use the save_main_session option because one or more DoFn's in this
#     # workflow rely on global context (e.g., a module imported at module level).
#     pipeline_options = PipelineOptions(pipeline_args)
#     pipeline_options.view_as(SetupOptions).save_main_session = save_main_session
#     pipeline_options.view_as(DirectOptions).direct_num_workers = 0
# 
#     # The pipeline will be run on exiting the with block.
#     with beam.Pipeline(options=pipeline_options) as p:
# 
#         # Read the text file[pattern] into a PCollection.
#         raw_data = p | "ReadTwitterData" >> ReadFromText(
#             known_args.input, coder=CustomCoder("latin-1")
#         )
# 
#         if known_args.mode == "train":
# 
#             transformed_data = (
#                 raw_data
#                 | "ExtractColumns" >> beam.ParDo(ExtractColumnsDoFn())
#                 | "Preprocess" >> beam.ParDo(PreprocessColumnsTrainFn())
#             )
# 
#             eval_percent = 20
#             assert 0 < eval_percent < 100, "eval_percent must in the range (0-100)"
#             train_dataset, eval_dataset = (
#                 transformed_data
#                 | "Split dataset"
#                 >> beam.Partition(
#                     lambda elem, _: int(random.uniform(0, 100) < eval_percent), 2
#                 )
#             )
# 
#             train_dataset | "TrainWriteToCSV" >> WriteToText(
#                 os.path.join(known_args.output, "train", "part")
#             )
#             eval_dataset | "EvalWriteToCSV" >> WriteToText(
#                 os.path.join(known_args.output, "eval", "part")
#             )
# 
#         else:
#             transformed_data = (
#                 raw_data
#                 | "ExtractColumns" >> beam.ParDo(ExtractColumnsDoFn())
#                 | "Preprocess" >> beam.Map(lambda x: f'"{x[0]}"')
#             )
# 
#             transformed_data | "TestWriteToCSV" >> WriteToText(
#                 os.path.join(known_args.output, "test", "part")
#             )
# 
# 
# if __name__ == "__main__":
#     logging.getLogger().setLevel(logging.INFO)
#     run()

"""Se proporciona un fichero `setup.py` necesario para ejecutar en DataFlow. Modificar la variable `REQUIRED_PACKAGES` con las dependencias que se hayan usado en el `requirements.txt`"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile setup.py
# 
# import setuptools
# 
# REQUIRED_PACKAGES = [
#   "apache-beam[gcp]==2.24.0",
#   "tensorflow==2.8.0",
#   "gensim==3.6.0",
#   "fsspec==0.8.4",
#   "gcsfs==0.7.1",
#   "numpy==1.20.0",
# ]
# 
# setuptools.setup(
#     name="twitchstreaming",
#     version="0.0.1",
#     install_requires=REQUIRED_PACKAGES,
#     packages=setuptools.find_packages(),
#     include_package_data=True,
#     description="Troll detection",
# )
#

"""### Validación preprocess train en local (0.25 puntos)

Con el comando mostrado a continuación se valida la correcta generación de los datos de entrenamiento y validación en local.
"""

! python3 preprocess.py \
  --work-dir $WORK_DIR \
  --runner DirectRunner \
  --input $WORK_DIR/data.json \
  --output $WORK_DIR/transformed_data \
  --mode train

"""### Validación preprocess test en local (0.25 puntos)

Con el comando mostrado a continuación se valida la correcta generación de los datos de test en local.
"""

! python3 preprocess.py \
  --work-dir $WORK_DIR \
  --runner DirectRunner \
  --input $WORK_DIR/data.json \
  --output $WORK_DIR/transformed_data \
  --mode test

"""## Entregable 2 (1.25 puntos)

Desarrollar una tarea de entrenamiento para los datos preprocesados. Requisitos:

- Soportar ejecuciones en local usando el SDK de AI-Platform y ejecuciones en GCP con el mismo código.

Se crea el directorio donde se dejará este entregable.
"""

# Commented out IPython magic to ensure Python compatibility.
# %mkdir /content/batch/trainer

# Commented out IPython magic to ensure Python compatibility.
# %%writefile trainer/__init__.py
# 
# version = "0.1.0"

# Commented out IPython magic to ensure Python compatibility.
# %%writefile trainer/task.py
# 
# from __future__ import absolute_import
# 
# import argparse
# import multiprocessing as mp
# import logging
# import tempfile
# import os
# 
# import pickle
# import gensim
# import pandas as pd
# import numpy as np
# import tensorflow as tf
# 
# from tensorflow.keras.preprocessing.text import Tokenizer
# from tensorflow.keras.preprocessing.sequence import pad_sequences
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import (
#     Dense,
#     Dropout,
#     Embedding,
#     LSTM,
# )
# from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping
# from sklearn.preprocessing import LabelEncoder
# 
# 
# # WORD2VEC
# W2V_SIZE = 300
# W2V_WINDOW = 7
# # 32
# W2V_EPOCH = 5
# W2V_MIN_COUNT = 10
# 
# # KERAS
# SEQUENCE_LENGTH = 300
# 
# # SENTIMENT
# PERSON = "PERSON"
# TROLL = "TROLL"
# 
# # EXPORT
# KERAS_MODEL = "model.h5"
# WORD2VEC_MODEL = "model.w2v"
# TOKENIZER_MODEL = "tokenizer.pkl"
# ENCODER_MODEL = "encoder.pkl"
# 
# 
# def generate_word2vec(train_df):
#     documents = [_text.split() for _text in train_df.text.values]
#     w2v_model = gensim.models.word2vec.Word2Vec(
#         size=W2V_SIZE,
#         window=W2V_WINDOW,
#         min_count=W2V_MIN_COUNT,
#         workers=mp.cpu_count(),
#     )
#     w2v_model.build_vocab(documents)
# 
#     words = w2v_model.wv.vocab.keys()
#     vocab_size = len(words)
#     logging.info(f"Vocab size: {vocab_size}")
#     w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)
# 
#     return w2v_model
# 
# 
# def generate_tokenizer(train_df):
#     tokenizer = Tokenizer()
#     tokenizer.fit_on_texts(train_df.text)
#     vocab_size = len(tokenizer.word_index) + 1
#     logging.info(f"Total words: {vocab_size}")
#     return tokenizer, vocab_size
# 
# 
# def generate_label_encoder(train_df):
#     encoder = LabelEncoder()
#     encoder.fit(train_df.sentiment.tolist())
#     return encoder
# 
# 
# def generate_embedding(word2vec_model, vocab_size, tokenizer):
#     embedding_matrix = np.zeros((vocab_size, W2V_SIZE))
#     for word, i in tokenizer.word_index.items():
#         if word in word2vec_model.wv:
#             embedding_matrix[i] = word2vec_model.wv[word]
#     return Embedding(
#         vocab_size,
#         W2V_SIZE,
#         weights=[embedding_matrix],
#         input_length=SEQUENCE_LENGTH,
#         trainable=False,
#     )
# 
# 
# def train_and_evaluate(
#     work_dir, train_df, eval_df, batch_size=1024, epochs=8, steps=1000
# ):
# 
#     """
#     Trains and evaluates the estimator given.
#     The input functions are generated by the preprocessing function.
#     """
# 
#     model_dir = os.path.join(work_dir, "model")
#     if tf.io.gfile.exists(model_dir):
#         tf.io.gfile.rmtree(model_dir)
#     tf.io.gfile.mkdir(model_dir)
# 
#     # Specify where to store our model
#     run_config = tf.estimator.RunConfig()
#     run_config = run_config.replace(model_dir=model_dir)
# 
#     # This will give us a more granular visualization of the training
#     run_config = run_config.replace(save_summary_steps=10)
# 
#     # Create Word2vec of training data
#     logging.info("---- Generating word2vec model ----")
#     word2vec_model = generate_word2vec(train_df)
# 
#     # Tokenize training data
#     logging.info("---- Generating tokenizer ----")
#     tokenizer, vocab_size = generate_tokenizer(train_df)
# 
#     logging.info("---- Tokenizing train data ----")
#     x_train = pad_sequences(
#         tokenizer.texts_to_sequences(train_df.text), maxlen=SEQUENCE_LENGTH
#     )
#     logging.info("---- Tokenizing eval data ----")
#     x_eval = pad_sequences(
#         tokenizer.texts_to_sequences(eval_df.text), maxlen=SEQUENCE_LENGTH
#     )
# 
#     # Label Encoder
#     logging.info("---- Generating label encoder ----")
#     label_encoder = generate_label_encoder(train_df)
# 
#     logging.info("---- Encoding train target ----")
#     y_train = label_encoder.transform(train_df.sentiment.tolist())
#     logging.info("---- Encoding eval target ----")
#     y_eval = label_encoder.transform(eval_df.sentiment.tolist())
# 
#     y_train = y_train.reshape(-1, 1)
#     y_eval = y_eval.reshape(-1, 1)
# 
#     # Create Embedding Layer
#     logging.info("---- Generating embedding layer ----")
#     embedding_layer = generate_embedding(word2vec_model, vocab_size, tokenizer)
# 
#     logging.info("---- Generating Sequential model ----")
#     model = Sequential()
#     model.add(embedding_layer)
#     model.add(Dropout(0.5))
#     model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
#     model.add(Dense(1, activation="sigmoid"))
# 
#     model.summary()
# 
#     logging.info("---- Adding loss function to model ----")
#     model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])
# 
#     logging.info("---- Adding callbacks to model ----")
#     callbacks = [
#         ReduceLROnPlateau(monitor="val_loss", patience=5, cooldown=0),
#         EarlyStopping(monitor="val_accuracy", min_delta=1e-4, patience=5),
#     ]
# 
#     logging.info("---- Training model ----")
#     model.fit(
#         x_train,
#         y_train,
#         batch_size=batch_size,
#         steps_per_epoch=steps,
#         epochs=epochs,
#         validation_split=0.1,
#         verbose=1,
#         callbacks=callbacks,
#     )
# 
#     logging.info("---- Evaluating model ----")
#     score = model.evaluate(x_eval, y_eval, batch_size=batch_size)
#     logging.info(f"ACCURACY: {score[1]}")
#     logging.info(f"LOSS: {score[0]}")
# 
#     logging.info("---- Saving models ----")
#     pickle.dump(
#         tokenizer,
#         tf.io.gfile.GFile(os.path.join(model_dir, TOKENIZER_MODEL), mode="wb"),
#         protocol=0,
#     )
#     with tempfile.NamedTemporaryFile(suffix=".h5") as local_file:
#         with tf.io.gfile.GFile(
#             os.path.join(model_dir, KERAS_MODEL), mode="wb"
#         ) as gcs_file:
#             model.save(local_file.name)
#             gcs_file.write(local_file.read())
# 
#     # word2vec_model.save(os.path.join(model_dir, WORD2VEC_MODEL))
# 
#     # pickle.dump(
#     #     label_encoder, open(os.path.join(model_dir, ENCODER_MODEL), "wb"), protocol=0
#     # )
# 
# 
# if __name__ == "__main__":
# 
#     """Main function called by AI Platform."""
# 
#     logging.getLogger().setLevel(logging.INFO)
# 
#     parser = argparse.ArgumentParser(
#         formatter_class=argparse.ArgumentDefaultsHelpFormatter
#     )
# 
#     parser.add_argument(
#         "--job-dir",
#         help="Directory for staging trainer files. "
#         "This can be a Google Cloud Storage path.",
#     )
# 
#     parser.add_argument(
#         "--work-dir",
#         required=True,
#         help="Directory for staging and working files. "
#         "This can be a Google Cloud Storage path.",
#     )
# 
#     parser.add_argument(
#         "--batch-size",
#         type=int,
#         default=1024,
#         help="Batch size for training and evaluation.",
#     )
# 
#     parser.add_argument(
#         "--epochs", type=int, default=8, help="Number of epochs to train the model",
#     )
# 
#     parser.add_argument(
#         "--steps",
#         type=int,
#         default=1000,
#         help="Number of steps per epoch to train the model",
#     )
# 
#     args = parser.parse_args()
# 
#     train_data_files = tf.io.gfile.glob(
#         os.path.join(args.work_dir, "transformed_data/train/part-*")
#     )
#     eval_data_files = tf.io.gfile.glob(
#         os.path.join(args.work_dir, "transformed_data/eval/part-*")
#     )
# 
#     train_df = pd.concat(
#         [
#             pd.read_csv(
#                 f,
#                 names=["text", "sentiment"],
#                 dtype={"text": "string", "sentiment": "string"},
#             )
#             for f in train_data_files
#         ]
#     ).dropna()
# 
#     eval_df = pd.concat(
#         [
#             pd.read_csv(
#                 f,
#                 names=["text", "sentiment"],
#                 dtype={"text": "string", "sentiment": "string"},
#             )
#             for f in eval_data_files
#         ]
#     ).dropna()
# 
#     train_and_evaluate(
#         args.work_dir,
#         train_df=train_df,
#         eval_df=eval_df,
#         batch_size=args.batch_size,
#         epochs=args.epochs,
#         steps=args.steps,
#     )
#

# Commented out IPython magic to ensure Python compatibility.
# he tenido que crear el directorio /model a mano
# porque no se me generaba con el codigo implementado
# %mkdir /content/batch/model
#%mkdir /content/batch/data/model

"""### Validación Train en local

Con el comando mostrado a continuación se valida el correcto entrenamiento del modelo usando los datos preprocesados del apartado anterior.
"""

# Explicitly tell `gcloud ai-platform local train` to use Python 3 
! gcloud config set ml_engine/local_python $(which python3)

# This is similar to `python -m trainer.task --job-dir local-training-output`
# but it better replicates the AI Platform environment, especially for
# distributed training (not applicable here).
! gcloud ai-platform local train \
  --package-path trainer \
  --module-name trainer.task \
  -- \
  --work-dir $WORK_DIR \
  --epochs 1 \
  #--steps 16

"""## Entregable 3 (0.5 puntos)

Desarrollar un pipeline de inferencia utilizando Apache Beam para generar predicciones usando los modelos generados en el apartado anterior así como los de test generados en el primer entregable.

"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile predict.py
# 
# from __future__ import absolute_import
# from __future__ import print_function
# 
# import argparse
# import tempfile
# import json
# import os
# import sys
# import time
# 
# import apache_beam as beam
# from apache_beam.io import ReadFromText
# from apache_beam.io import WriteToText
# from apache_beam.options.pipeline_options import GoogleCloudOptions
# from apache_beam.options.pipeline_options import PipelineOptions
# from apache_beam.options.pipeline_options import SetupOptions
# from apache_beam.coders.coders import Coder
# 
# import pickle
# import tensorflow as tf
# from tensorflow.keras.preprocessing.sequence import pad_sequences
# 
# # KERAS
# SEQUENCE_LENGTH = 300
# 
# # SENTIMENT
# PERSON = "PERSON"
# TROLL = "TROLL"
# 
# # EXPORT
# KERAS_MODEL = "model.h5"
# TOKENIZER_MODEL = "tokenizer.pkl"
# 
# 
# class Predict(beam.DoFn):
#     def __init__(
#         self, model_dir,
#     ):
#         self.model_dir = model_dir
#         self.model = None
#         self.tokenizer = None
# 
#     def setup(self):
#         keras_model_path = os.path.join(self.model_dir, KERAS_MODEL)
#         with tempfile.NamedTemporaryFile(suffix=".h5") as local_file:
#             with tf.io.gfile.GFile(keras_model_path, mode="rb") as gcs_file:
#                 local_file.write(gcs_file.read())
#                 self.model = tf.keras.models.load_model(local_file.name)
# 
#         tokenizer_path = os.path.join(self.model_dir, TOKENIZER_MODEL)
#         self.tokenizer = pickle.load(tf.io.gfile.GFile(tokenizer_path, mode="rb"))
# 
#     def decode_sentiment(self, score):
#       label = TROLL
#       if score <= 0.5:
#           label = PERSON
#       return label
# 
#     def process(self, element):
#         start_at = time.time()
#         # Tokenize text
#         x_test = pad_sequences(
#             self.tokenizer.texts_to_sequences([element]), maxlen=SEQUENCE_LENGTH
#         )
#         # Predict
#         score = self.model.predict([x_test])[0]
#         # Decode sentiment
#         label = self.decode_sentiment(score)
# 
#         yield {
#             "text": element,
#             "label": label,
#             "score": float(score),
#             "elapsed_time": time.time() - start_at,
#         }
# 
# 
# class CustomCoder(Coder):
#     """A custom coder used for reading and writing strings"""
# 
#     def __init__(self, encoding: str):
#         # latin-1
#         # iso-8859-1
#         self.enconding = encoding
# 
#     def encode(self, value):
#         return value.encode(self.enconding)
# 
#     def decode(self, value):
#         return value.decode(self.enconding)
# 
#     def is_deterministic(self):
#         return True
# 
# 
# def run(model_dir, source, sink, beam_options=None):
#     with beam.Pipeline(options=beam_options) as p:
#         _ = (
#             p
#             | "Read data" >> source
#             # | "Preprocess" >> beam.ParDo(PreprocessTextFn(model_dir, "ID"))
#             | "Predict" >> beam.ParDo(Predict(model_dir))
#             | "Format as JSON" >> beam.Map(json.dumps)
#             | "Write predictions" >> sink
#         )
# 
# 
# if __name__ == "__main__":
#     """Main function"""
#     parser = argparse.ArgumentParser(
#         formatter_class=argparse.ArgumentDefaultsHelpFormatter
#     )
# 
#     parser.add_argument(
#         "--work-dir",
#         dest="work_dir",
#         required=True,
#         help="Directory for temporary files and preprocessed datasets to. "
#         "This can be a Google Cloud Storage path.",
#     )
# 
#     parser.add_argument(
#         "--model-dir",
#         dest="model_dir",
#         required=True,
#         help="Path to the exported TensorFlow model. "
#         "This can be a Google Cloud Storage path.",
#     )
# 
#     verbs = parser.add_subparsers(dest="verb")
#     batch_verb = verbs.add_parser("batch", help="Batch prediction")
#     batch_verb.add_argument(
#         "--inputs-dir",
#         dest="inputs_dir",
#         required=True,
#         help="Input directory where CSV data files are read from. "
#         "This can be a Google Cloud Storage path.",
#     )
#     batch_verb.add_argument(
#         "--outputs-dir",
#         dest="outputs_dir",
#         required=True,
#         help="Directory to store prediction results. "
#         "This can be a Google Cloud Storage path.",
#     )
# 
#     args, pipeline_args = parser.parse_known_args()
#     print(args)
#     beam_options = PipelineOptions(pipeline_args)
#     beam_options.view_as(SetupOptions).save_main_session = True
#     # beam_options.view_as(DirectOptions).direct_num_workers = 0
# 
#     project = beam_options.view_as(GoogleCloudOptions).project
# 
#     if args.verb == "batch":
#         results_prefix = os.path.join(args.outputs_dir, "part")
# 
#         source = ReadFromText(args.inputs_dir, coder=CustomCoder("latin-1"))
#         sink = WriteToText(results_prefix)
# 
#     else:
#         parser.print_usage()
#         sys.exit(1)
# 
#     run(args.model_dir, source, sink, beam_options)
#

"""Generamos un timestamp para la ejecución de las predicciones"""

from datetime import datetime

# current date and time
TIMESTAMP = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')

"""### Validación Predict en local

Con el comando mostrado a continuación se valida la correcta inferencia usando los modelos anteriores y los datos de test generados anteriormente.
"""

! python3 predict.py \
  --work-dir $WORK_DIR \
  --model-dir $WORK_DIR/model \
  batch \
  --inputs-dir $WORK_DIR/transformed_data/test/part* \
  --outputs-dir $WORK_DIR/predictions/$TIMESTAMP

"""##Entregable 4 (1.25 puntos)

En este entregable se validará el funcionamiento del código en un proyecto de GCP sobre DataFlow y AI Platform

Establecemos el bucket y region de GCP sobre el que trabajaremos:
"""

GCP_WORK_DIR = 'gs://final-practice-test-execution'
GCP_REGION = 'us-east1'

"""### Validación preprocess train en Dataflow (0.25 puntos)

Con el comando mostrado a continuación se valida la correcta generación de los datos de entrenamiento y validación en GCP con el servicio DataFlow.
"""

! python3 preprocess.py \
  --project $PROJECT_ID \
  --region $GCP_REGION \
  --runner DataflowRunner \
  --temp_location $GCP_WORK_DIR/beam-temp \
  --setup_file ./setup.py \
  --work-dir $GCP_WORK_DIR \
  --input $GCP_WORK_DIR/data.json \
  --output $GCP_WORK_DIR/transformed_data \
  --mode train

"""### Validación preprocess test en Dataflow (0.25 puntos)

Con el comando mostrado a continuación se valida la correcta generación de los datos de test en GCP con el servicio DataFlow.
"""

! python3 preprocess.py \
  --project $PROJECT_ID \
  --region $GCP_REGION \
  --runner DataflowRunner \
  --temp_location $GCP_WORK_DIR/beam-temp \
  --setup_file ./setup.py \
  --work-dir $GCP_WORK_DIR \
  --input $GCP_WORK_DIR/data.json \
  --output $GCP_WORK_DIR/transformed_data \
  --mode test

"""### Validación Train en AI Platform (0.5 puntos)

Con el comando mostrado a continuación se valida el correcto entrenamiento del modelo usando los datos de las ejecuciones anteriores en GCP con los datos obtenidos almacenados en Google Cloud Storage.

Generamos un nombre para el job de entrenamiento y donde se almacenarán los metadatos.
"""

JOB = "troll_detection_batch_$(date +%Y%m%d_%H%M%S)"
JOB_DIR = GCP_WORK_DIR + "/trainer"

! gcloud ai-platform jobs submit training $JOB \
  --module-name trainer.task \
  --package-path trainer \
  --scale-tier basic_gpu \
  --python-version 3.7 \
  --runtime-version 2.1 \
  --region $GCP_REGION \
  --job-dir $JOB_DIR \
  --stream-logs \
  -- \
  --work-dir $GCP_WORK_DIR \
  --epochs 1

"""### Validación predict en Dataflow (0.25 puntos)

Con el comando mostrado a continuación se valida la predicción correcta de los datos de test usando los modelos generados en el comando anterior.

Generamos un timestamp para el almacenamiento de las inferencias en Google Cloud Storage.
"""

from datetime import datetime

# current date and time
TIMESTAMP = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')

# For using sample models: --model-dir gs://$BUCKET_NAME/models/
! python3 predict.py \
  --work-dir $GCP_WORK_DIR \
  --model-dir $GCP_WORK_DIR/model/ \
  batch \
  --project $PROJECT_ID \
  --region $GCP_REGION \
  --runner DataflowRunner \
  --temp_location $GCP_WORK_DIR/beam-temp \
  --setup_file ./setup.py \
  --inputs-dir $GCP_WORK_DIR/transformed_data/test/part* \
  --outputs-dir $GCP_WORK_DIR/predictions/$TIMESTAMP

"""# Inferencia online

En esta segunda parte de la práctica se realizará un microservicio de inferencia online usando los modelos generados en la primera parte. Para esta parte de la práctica el código de vuestro microservicio deberá estar subido en un repositorio. En la variable de debajo deberéis dejar la URL a vuestro repositorrio pues será el contenido con el que serás evaluado. 

**Importante:** asegúrate de crear el repositorio de manera pública para poder clonarlo.

A continuación os dejo un diagrama con la arquitectura que se va a desarrollar:

![online_diagram](https://drive.google.com/uc?export=view&id=1zR7Cwp0Vq1QeTxwLoJ8YJNRM9G5KVh2S)
"""

REPOSITORIO = "https://github.com/jorge-melgosa/KCBDML9_espliegue-de-algoritmos.git"

#subir todos los archivos al repositorio que creemos para la practica

# enumos: cambiar la estructura
# servises modelos: cambiar la parte de sentimiento similar a batch
# si cambiamos el nombre del enum, cambiar en prediction

"""Creamos el directorio donde trabajaremos."""

# Commented out IPython magic to ensure Python compatibility.
# %mkdir /content/online
# %cd /content/online

# Commented out IPython magic to ensure Python compatibility.
# Clone the repository
! git clone $REPOSITORIO

# Set the working directory to the sample code directory
# %cd ./KCBDML9_espliegue-de-algoritmos

# Change to develop
! git checkout develop

! pip install -r requirements.txt

! pip install pyngrok

"""Para cuando estes modificando, probando y ejecutando ficheros os dejo en las celdas de abajo los comandos de git necesarios para interaccionar con vuestro repositorio en caso de que queráis:"""

! git status

! git add <files>
! git commit -m "Nuevos cambios"
! git push origin master

"""Será necesario definir y establecer la variable de entorno `DEFAULT_MODEL_PATH` para definir donde están almacenados nuestros modelos para hacer inferencia."""

import os

os.environ["DEFAULT_MODEL_PATH"] = "/content/batch/model/"

"""### Validación inferencia online en local (1.75 puntos)

Se validará la correcta inferencia del microservio en local utilizando Swagger. Para ejecutar en local solo hay que ejecutar los comandos a continuación. Después, entrar en la URL proporcionada por ngrock `<ngrok_url>/docs` para acceder a swagger y probar la inferencia como vimos en clase.
"""

# For testing purposes
import nest_asyncio
from pyngrok import ngrok, conf

conf.get_default().auth_token = "2BfKAeBaVI7DyxRZ9oS2Mg2nKpX_ZftpND2RQXobBNdX33SC"

ngrok_tunnel = ngrok.connect(8000)
print('Public URL:', ngrok_tunnel.public_url)
nest_asyncio.apply()

! uvicorn app.main:app --port 8000

"""### Validación inferencia online en GCP (1.75 puntos)

Se validará el correcto funcionamiento del microservicio haciendo una petición POST de inferencia a través de curl al microservicio desplegado en GCP.

Primero, contruiremos una imagen Docker con el microservicio y subiremos el desarrollo al Container Repository en GCP a través de Cloud Build.
"""

! gcloud builds submit --tag gcr.io/$PROJECT_ID/troll-detection-online-service

"""Desplegaremos la imagen Docker generada en el Container Registry en el servicio de Cloud Run. Después, validaremos que las inferencias funcionan en GCP usando el comando mostrado a continuación:"""

! curl -X POST "https://troll-detection-service-g2v7skqfeq-uc.a.run.app/api/model/predict" -H  "accept: application/json" -H  "Content-Type: application/json" -d "{\"text\":\"i hate you\"}"

"""# Detección de Trolls en Twitch en Streaming

En esta última parte de la práctica se realizará un pipeline de inferencia en tiempo real de un chat de Twitch alcualmente en vivo. Para ello, usaremos mi canal de Twitch `https://www.twitch.tv/franalgaba` donde tengo un bot deplegado poniendo mensajes troll y no troll de forma aleatoria del dataset que hemos usado en la primera parte.

Para acceder al chat de Twitch os proporciono el conector correspondiente que será desplegado como Cloud Function como hicimos en clase y usando mis credenciales recogerá los mensajes del chat y los enviará a un topic de Pub/Sub en GCP. Después, desarrollarás un job en streaming de Dataflow con el que leerás esos mensajes de Pub/sub, los mandarás a tu microservicio de inferencia para que haga las predicciones y enviarás los resultados a un nuevo tópico de Pub/Sub.

A continuación os dejo un diagrama con la arquitectura que se va a desarrollar:

![streaming_diagram](https://drive.google.com/uc?export=view&id=1TEBPPc9ZF09IM5iGq9FwGAx9PVzAYNPg)

Primero, creamos el publisher que será el encargado de recoger los mensajes de Twitch y enviarlos a Pub/Sub. Esto os lo doy yo desarrollado, sólo tendréis que desplegarlo en una Cloud Function.
"""

# Commented out IPython magic to ensure Python compatibility.
# %mkdir -p /content/streaming/publisher

# Commented out IPython magic to ensure Python compatibility.
# Execute after restart
# %cd /content/streaming/publisher

# Commented out IPython magic to ensure Python compatibility.
# %%writefile requirements.txt
# 
# twitchio==1.2.3
# loguru==0.5.3
# google-cloud-pubsub==2.1.0

# Commented out IPython magic to ensure Python compatibility.
# %%writefile main.py
# 
# import os  # for importing env vars for the bot to use
# import sys
# import json
# import time
# 
# from twitchio.ext import commands
# from google.cloud import pubsub_v1
# from loguru import logger
# 
# PROJECT_ID = os.getenv("PROJECT_ID")
# TOPIC_NAME = os.getenv("TOPIC_NAME")
# 
# TOPIC_PATH = f"projects/{PROJECT_ID}/topics/{TOPIC_NAME}"
# 
# publisher = pubsub_v1.PublisherClient()
# 
# 
# class Bot(commands.Bot):
# 
#     def __init__(self, irc_token='...', client_id='...', nick='...', prefix="!", initial_channels=['...'], debug=True):
#         super().__init__(irc_token=irc_token, client_id=client_id, nick=nick, prefix='!',
#                          initial_channels=initial_channels)
#         self.debug = debug
# 
#     # Events don't need decorators when subclassed
#     async def event_ready(self):
#         logger.info('Ready')
# 
#     async def event_message(self, message):
#         logger.info(message.content)
#         publisher.publish(TOPIC_PATH, str.encode(message.content))
# 
# 
# def main(request):
# 
#     topic_name = f"projects/{PROJECT_ID}/topics/{TOPIC_NAME}"
#     # publisher.create_topic(topic_name)
# 
#     request_json = request.get_json(silent=True)
# 
#     logger.info("Starting listener...")
#     if "debug" in request_json and isinstance(request_json["debug"], bool):
#         logger.info(f"Debug mode: {request_json['debug']}")
#         bot = Bot(
#           # set up the bot
#           irc_token="oauth:xl5cpf8qe8tl1d03dppymchi6r04iz",
#           client_id="ciliqxi534iwg4pfqj7swl1jmkt23y",
#           nick="franalgaba",
#           prefix="!",
#           initial_channels=["franalgaba"],
#           debug=request_json['debug'])
#     else:
#         bot = Bot(
#           # set up the bot
#           irc_token="oauth:xl5cpf8qe8tl1d03dppymchi6r04iz",
#           client_id="ciliqxi534iwg4pfqj7swl1jmkt23y",
#           nick="franalgaba",
#           prefix="!",
#           initial_channels=["franalgaba"])
# 
#     bot.run()

# In case user service error...
! gcloud iam service-accounts add-iam-policy-binding kc-mel-practica-da@appspot.gserviceaccount.com --member=user:<mail> --role=roles/iam.serviceAccountUser

"""Para lanzar vuestra Cloud Function, que recoja y mande mensajes solo tenéis que ejecutar el comando siguiente (haced los pasos vistos en clase para desplegar el servicio):"""

! curl -X POST https://europe-west1-kc-mel-practica-da.cloudfunctions.net/twitch-published -H "Content-Type:application/json"  -d '{"debug": false}'

"""## Entregable 1 (3 puntos)

En este entregable desarrollarás un pipeline de inferencia en streaming usando Apache Beam para ejecutar en Dataflow un job en streaming que llamará a vuestro microservicio para realizar inferencias.
"""

# Commented out IPython magic to ensure Python compatibility.
# %mkdir /content/streaming/subscriber

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/streaming/subscriber

# Commented out IPython magic to ensure Python compatibility.
# %%writefile requirements.txt
# 
# apache-beam[gcp]==2.24.0
# fsspec==0.8.4
# gcsfs==0.7.1
# loguru==0.5.3

! pip install -r requirements.txt

# Commented out IPython magic to ensure Python compatibility.
# %%writefile predict.py
# 
# from __future__ import absolute_import
# from __future__ import print_function
# 
# import argparse
# import requests
# import json
# import sys
# 
# import apache_beam as beam
# import apache_beam.transforms.window as window
# from apache_beam.options.pipeline_options import (
#     GoogleCloudOptions,
#     StandardOptions,
#     PipelineOptions,
#     SetupOptions,
# )
# from loguru import logger
# 
# 
# class Predict(beam.DoFn):
#     def __init__(self, predict_server) -> None:
#         self.url = predict_server
# 
#     def _predict(self, text) -> str:
#         payload = {"text": text}
#         headers = {"accept": "application/json", "Content-Type": "application/json"}
#         try:
#             response = requests.post(
#                 self.url, data=json.dumps(payload), headers=headers
#             )
#             response = json.loads(response.text)
#         except Exception:
#             response = {"label": "undefined", "score": 0, "elapsed_time": 0}
# 
#         return response
# 
#     def process(self, element, window=beam.DoFn.WindowParam):
#         logger.info(f"Text to predict: {element}")
#         result = self._predict(element)
#         result["text"] = element
#         yield json.dumps(result)
# 
# 
# def run(predict_server, source, sink, beam_options=None):
#     with beam.Pipeline(options=beam_options) as p:
#         _ = (
#             p
#             | "Read data from PubSub" >> source
#             | "decode" >> beam.Map(lambda x: x.decode("utf-8"))
#             | "window" >> beam.WindowInto(window.FixedWindows(15))
#             | "Predict" >> beam.ParDo(Predict(predict_server))
#             | "encode" >> beam.Map(lambda x: x.encode("utf-8")).with_output_types(bytes)
#             | "Write predictions" >> sink
#         )
# 
# 
# if __name__ == "__main__":
#     """Main function"""
#     parser = argparse.ArgumentParser(
#         formatter_class=argparse.ArgumentDefaultsHelpFormatter
#     )
# 
#     parser.add_argument(
#         "--inputs_topic",
#         dest="inputs_topic",
#         required=True,
#         help="Directory for temporary files and preprocessed datasets to. "
#         "This can be a Google Cloud Storage path.",
#     )
# 
#     parser.add_argument(
#         "--outputs_topic",
#         dest="outputs_topic",
#         required=True,
#         help="Directory for temporary files and preprocessed datasets to. "
#         "This can be a Google Cloud Storage path.",
#     )
# 
#     parser.add_argument(
#         "--predict_server",
#         dest="predict_server",
#         required=True,
#         help="Directory for temporary files and preprocessed datasets to. "
#         "This can be a Google Cloud Storage path.",
#     )
# 
#     args, pipeline_args = parser.parse_known_args()
#     logger.info(args)
#     beam_options = PipelineOptions(pipeline_args)
#     beam_options.view_as(SetupOptions).save_main_session = True
#     # beam_options.view_as(DirectOptions).direct_num_workers = 0
# 
#     project = beam_options.view_as(GoogleCloudOptions).project
# 
#     if not project:
#         parser.print_usage()
#         print("error: argument --project is required for streaming")
#         sys.exit(1)
# 
#     beam_options.view_as(StandardOptions).streaming = True
# 
#     source = beam.io.ReadFromPubSub(
#         topic="projects/{}/topics/{}".format(project, args.inputs_topic)
#     ).with_output_types(bytes)
# 
#     sink = beam.io.WriteToPubSub(
#         topic="projects/{}/topics/{}".format(project, args.outputs_topic)
#     )
# 
#     run(args.predict_server, source, sink, beam_options)
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile setup.py
# 
# import setuptools
# 
# REQUIRED_PACKAGES = [
#     "apache-beam[gcp]==2.24.0",
#     "fsspec==0.8.4",
#     "gcsfs==0.7.1",
#     "loguru==0.5.3",
# ]
# 
# setuptools.setup(
#     name="twitchstreaming",
#     version="0.0.1",
#     install_requires=REQUIRED_PACKAGES,
#     packages=setuptools.find_packages(),
#     include_package_data=True,
#     description="Twitch Troll Detection",
# )

"""### Validación inferencia en streaming

Con el comando mostrado a continuación se genera un job en streaming de Dataflow. Antes de ejecutarlo, deberás crear dos topicos en Pub/Sub, `twitch-chat` donde se recibirán los mensajes de twitch, y `twitch-chat-predictions` donde se mandarán las predicciones generadas por vuestro microservicio.

**Importante**: no te olvides de modificar la URL de tu microservicio de inferencia.
"""

GCP_WORK_DIR = 'gs://final-practice-test-execution'
GCP_REGION = 'us-east1'

https://troll-detection-online-service-gqlslm5ycq-no.a.run.app

! python3 predict.py \
--project $PROJECT_ID \
--region $GCP_REGION \
--runner DataflowRunner \
--temp_location $GCP_WORK_DIR/beam-temp \
--setup_file ./setup.py \
--inputs_topic twitch-chat \
--outputs_topic twitch-chat-predictions \
--predict_server https://troll-detection-online-service-gqlslm5ycq-no.a.run.app/api/model/predict \